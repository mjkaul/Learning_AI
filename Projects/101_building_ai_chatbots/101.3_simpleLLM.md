# Learning AI 101.2 - Train and deploy a simple LLM, then test it to find its weaknesses

## Learning Path

I borrowed this learning path is from [Sebastian Raschka](https://www.linkedin.com/posts/sebastianraschka_a-suggestion-for-an-effective-11-step-llm-activity-7195778889384693762-2TB_). I've been working through his book [*Building a Large Language Model (from Scratch)*](https://www.manning.com/books/build-a-large-language-model-from-scratch). It's excellentâ€”though perhaps quite advanced if you're new to both coding & AI. When he refers to "chapters" below", he's talking about the chapters in this book.

Let me emphasize, though: you don't need to be an expert coder to use Raschka's book. You can follow along in a Jupyter notebookâ€”just follow the instructions in the book. You'll be glad you started.

A suggestion for an effective 11-step LLM summer study plan:

1. [ ] Read* Chapters 1 and 2 on implementing the data loading pipeline (https://lnkd.in/gfruEiwmÂ &Â https://lnkd.in/gyDm4h3y).
2. [ ] Watch [Karpathy's video](https://www.youtube.com/watch?v=zduSFxRajkE) on training a BPE tokenizer from scratch.
3. [ ] Read Chapters 3 and 4 on implementing the model architecture.
4. [ ] Watch Karpathy's video on pretraining the LLM.
5. [ ] Read Chapter 5 on pretraining the LLM and then loading pretrained weights.
6. [ ] Read Appendix E on adding additional bells and whistles to the training loop.
7. [ ] Read Chapters 6 and 7 on finetuning the LLM.
8. [ ] Read Appendix E on parameter-efficient finetuning with LoRA.
9. [ ] Check out [Karpathy's repo]((https://github.com/karpathy/llm.c)) on coding the LLM in C code.
10. [ ] Check out [LitGPT](https://github.com/Lightning-AI/litgpt) to see how multi-GPU training is implemented and how different LLM architectures compare.
11. [ ] Build something cool and share it with the world.

(*"Read" = read, run the code, and attempt the exercises ðŸ˜Š)

## Objectives



## Concepts

